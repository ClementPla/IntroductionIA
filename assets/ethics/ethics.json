[
  {
    "title":"Biais et Inclusivité",
    "shortDescription":"Le Graal de l'objectivité algorithmique?",
    "longDescription":"La question de l'inclusivité est fondamentale dans la conception d'un algorithme quand celui-ci traite avec des humains. On s'imagine parfois à tort qu'un algorithme est nécessairement plus objectif que la décision d'un opérateur humain (dans la mesure où ses décisions proviendraient d'un ensemble de règles écrites en amont de manière rationnelle et dépassionnée). Or, de nombreuses expériences ont demontré le réel risque de reproduction de travers de société dans certaines prises de décisions algorithmiques (des modèles faisant preuve de sexisme, de racisme, d'homophobie...). Plusieurs raisons peuvent expliquer ces comportements. <p>Les modèles d'aujourd'hui sont le plus souvent conçus pour s'adapter au mieux à des données dites <b>d'apprentissage</b>. Cela peut engendrer deux types de problèmes: si ces données sont biaisées, l'algorithme le sera tout autant. Ainsi, entraîné sur des données discrimatoires, un modèle reproduira ces discriminations. Ainsi, des données explicitement racistes conduiront à un modèle qui risque de se reproduire ce comportement. Encore plus difficile à détecter et à corriger, si ces données ne représentent pas la société dans son exhaustivité, il sera difficile de prévoir comment l'algorithme se comportera face à des situations auxquelles il n'aura jamais été confronté. Plusieurs exemples relatifs aux biais d'apprentissage ont fait la couverture médiatique ces dernières années.</p> <p>En 2016, Microsoft lance un assistant conversationnel sur Twitter, pour qu'il y apprenne à converser avec les internautes. En l'espace de 24h, le modèle a progressivement commencé à publier des tweet misanthropes, des appels aux meurtres de militants féministes avant de se déclarer ouvertement nazi. Bien entendu, le modèle est lui-même incapable de comprendre ces notions: il ne faisait que répéter ce que les internautes lui avaient sciemment appris. </p> <p>La notion de biais n'est pas nécessairement lié à l'algorithme ou même à ses données. En effet, certaines discriminations algorithmiques ne sont que des reproductions parfois indirectes de réalités sociétales discriminatoires. En avril 2016, Amazon décida d'exclure des quartiers de banlieues américaines de son service de livraison gratuite en un jour. Cette décision fut prise suivant le calcul d'un modèle de bénéfices qui estimait que ces quartiers étaient trop pauvres pour que l'entreprise puisse espérer y faire des profits, la rentabilité n'étant pas au rendez-vous pour l'entreprise. Mais il s'agissait de quartiers essentiellement habités par des populations afro-américaines noires. Bien que n'étant entraîné qu'à partir de données financières et économiques, l'algorithme avait donc indirectement reproduit une forme de ségrégation raciale.</p> <p>Renforcée par la place que les modèles automatiques vont prendre, l'exigence de la prise en compte des biais de données n'est cependant pas spécifique aux applications informatiques. Par exemple, en pharmacologie, l'exigence de données représentant la population dans son ensemble (en terme d'ethnicités, de sexe, d'handicap...) est particulièrement importante. </p><p>Enfin, il existe de très sérieuses critiques quant à l'existence même des bases de données et leur contenu. Outre qu'un grand nombre d'entre elles -y compris parmi les plus importantes références académiques- se révèlent avoir été montées sans l'accord explicite des ayant-droits (quand il s'agit de photo de personnes), des enquêtes ont révélé l'incongruité des taxonomies qu'elles proposent parfois. Une photo d'un homme un verre de bière à la main? L'image porte pour label <i>alcoolique, ivrogne, luxure, dipsomane</i>. Un enfant qui porte des lunettes de soleil? Un <i>raté, loser, moins-que-rien</i>. Certaines catégories se révèlent encore bien plus problématiques et dans certains cas, l'existence même d'une catégorisation n'est pas un choix anodin. L'enquête de Kate Crawford et Trevor Paglen à ce sujet est <a href='https://excavating.ai/'>parfois absurdement comique, souvent effrayante.</a></p>",
    "imgUrl":"assets/ethics/images/gender-bias-person-people-inequality-svgrepo-com.svg",
    "url":"biais"
  },
  {
    "title":"Éthique formelle ou spécifique?",
    "shortDescription":"Un algorithme peut-il devenir éthique?",
    "longDescription":"Des algorithmes pouvant prendre des décisions capitales en toute autonomie interroge sur la notion d'éthique de la machine. La machine peut être en effet amenée à faire des choix basés sur un jugement de valeurs. Par exemple, quelle prise de décision doit-être celle de la voiture autonome face à un accident inéluctable? Faut-il protéger à tout prix les passagers de l'habitacle ou bien le piéton? On parle alors de formaliser la prise de décision éthique, c'est à dire prévoir les règles qu'appliquera la machine face à ce genre de situations. Derrière cette notion, deux problèmes se posent. <ul><li>Tout d'abord, il n'existe pas de garantie que chaque situation aura été prise en compte dans le <b>formalisme éthique</b> codé dans la machine. De nombreux problèmes éthiques concernent des situations inédites pour lesquelles il n'existe pas forcément de préalable sur lesquelles baser un jugement. La prise de décision revient alors aux valeurs propres de chacun, notion qu'il est difficile de définir pour une machine. On parle alors de spécificité de l'éthique.</li><li> La seconde difficulté liée à l'éthique formelle est celle du choix des personnes chargées d'établir et rédiger ces règles éthiques. En particulier en démocratie, le choix des règles ne peut pas être laissé qu'à la seule appréciation d'experts. La question de l'éducation et l'implication de la société face à ces problématiques est de ce fait particulièrement pressante.</li></ul>",
    "imgUrl":"assets/ethics/images/computer-surveillance-artificial-intelligence-computer-ai-svgrepo-com.svg",
    "url":"specificite"
  },
  {
    "title":"Fragmentation du collectif",
    "shortDescription":"L'hyperpersonnalisation peut-elle dissoudre le collectif?",
    "longDescription":"La notion de fragmentation du collectif est une conséquence indirecte de la haute performance des algorithmes, en particulier sur les réseaux sociaux. Grâce à des modèles de plus en plus sophistiqués, il est relativement facile de proposer un contenu extrêmement personnalisé à une cible donnée. L'objectif souvent revendiqué est de n'apporter que du contenu jugé (préalablement et automatiquement) pertinent et intéressant à l'utilisateur. Mais de nombreux effets néfastes ont été largement documentés et noircissent ce tableau. Le plus célèbre est celui de <b>bulle informative</b> théorisée par le militant Eli Pariser. Du fait de l'hyper-personnalisation du contenu sur internet, chaque internaute ne reçoit que du contenu (actualité, culture, politique) qui le conforte dans sa communauté ou son opinion. L'absence d'opinions contradictoires à l'échelle de l'individu peut alors fragmenter d'autant plus la société au détriment du pluralisme. <p>Un autre exemple de la fragmentation du collectif est la perte de la notion de mutualisation des dommages. Ce principe, à la base du modèle de l'assurance, permet d'amoindrir les coûts individuels en les reportant sur le collectif. Mais avec des modèles prédictifs de plus en plus sophistiqués, les compagnies d'assurance peuvent être tentées de proposer des formules personnalisées à chacun, provoquant un surcoût sur les personnes déjà vulnérables.</p>",
    "imgUrl":"assets/ethics/images/social-svgrepo-com.svg",
    "url":"fragmentation"
  },

  {
    "title":"Applications et usages",
    "shortDescription":"Faut-il limiter les usages d'IA?",
    "longDescription":"Les questionnements relatifs aux applications de la technologie n'est certainement pas propre à l'IA. Mais l'efficacité de celle-ci interroge sur l'encadrement de certaines pratiques. Bien entendu, les applications militaires, de surveillance de masse ou prédictions de comportement sont les premières visées par ces questions. Mais au delà, de nombreuses applications a priori bénéfiques peuvent révéler une face obscure. Fin 2020, la compagnie DeepMind publie un article accompagnant son modèle <i>AlphaFold 2</i>. <a href='https://www.nature.com/articles/s41591-021-01533-0'>Ce modèle est capable de reconstituer la structure d'une protéine à partir de sa séquence d'acide aminés avec une précision remarquable, surpassant largement les modèles existants</a>. Il s'agit d'une avancée majeure pour la biologie et la recherche thérapeutique. Mais inversement, en mars 2022, <a href='https://www.nature.com/articles/s42256-022-00465-9'>une étude révèle le potentiel de son modèle pour créer des agents chimiques toxiques</a>. En l'espace de 6 heures, le modèle génère plus de 40 000 nouvelles molécules potentiellement toxiques.",
    "imgUrl":"assets/ethics/images/aim-svgrepo-com.svg",
    "url":"applications"
  },
  {
    "title":"Autonomie",
    "shortDescription":"Perte de responsabilité ou trouble de l'expertise?",
    "longDescription":"Face à l'émergence d'algorithmes \"intelligents\", capables de réaliser des prédictions avec une fiabilité supérieure à l'humain, deux questions se posent. Dans certains domaines nécessitant un haut degré d'expertise et une forte prise de responsabilité, comment impliquer un modèle avec l'expert humain? En particulier, en cas de désaccord, à qui reviendra la responsabilité de trancher, et quelle responsabilité incombera à qui? Un radiologue prendra-t-il le risque de contredire un modèle réputé performant qui diagnostique une tumeur là où lui-même ne voit qu'une structure bénigne? <p>Derrière ce risque lié à la peur de l'erreur se dessine également un second problème, relatif à la perte de compétences. L'apparition de modèles \"sachant tout faire\"  peut très bien conduire à une déresponsabilisation et in fine à une perte d'expertise. Qui pratique le calcul mental lorsqu'une calculatrice sera toujours plus efficace? Utilise une carte routière à l'heure du GPS? <i>Va chez le médecin à l'heure du diagnostic automatisé réalisé par une IA sur son portable?</i></p>",
    "imgUrl":"assets/ethics/images/artificial-intelligence-svgrepo-com.svg",
    "url":"autonomie"
  },
  {
    "title":"Fiabilité et interprétabilité",
    "shortDescription":"Comment faire confiance à ce qu'on ne comprend pas?",
    "longDescription":"Une des difficultés des réseaux de neurones artificielles est liée à leur complexité. Certains modèles peuvent posséder des centaines de millions (voire des milliards) de paramètres, ce qui dépasse très largement l'entendement humain. De ce fait, on parle souvent de ces modèles comme étant des boîtes noires un peu magiques, dont on sait qu'elles fonctionnent, sans être capable d'expliquer comment. Cela pose plusieurs problèmes. Le premier est celui de la confiance que l'on peut accorder à cette boîte noire. Pour cela, un champs de recherche très actif est celui de l'interprétabilité. Si on ne peut comprendre l'algorithme, celui-ci aura la tâche de s'expliquer, c'est à dire indiquer comment il a fait sa prédiction d'une manière que l'humain puisse comprendre. Par ailleurs, la question de la fiabilité du modèle est également primordiale. Comme tout système informatique, les réseaux de neurones peuvent être piratés. En particulier, l'exemple de l'attaque adversariale est particulièrement intéressant en ce qui concerne les réseaux de neurones. Cette attaque consiste à modifier de manière imperceptible (pour un humain) un objet de manière à entièrement modifier la prédiction d'un réseau de neurones. Sur l'exemple ci-dessous, ci-dessous, en modifiant très légèrement une image de cochon ou de pandas (initialement correctement reconnues par le modèle), il a été possible de tromper le modèle jusqu'à ce qu'il finisse par y voir un avion de ligne (image de droite). Or, cette modification est complètement imperceptible à l'oeil humain.<img src='assets/others/adversarial_attack.png'>",
    "imgUrl":"assets/ethics/images/open-black-box-svgrepo-com.svg",
    "url":"fiabilite"
  }
]
