[
  {
    "title":"Biais et Inclusivité",
    "shortDescription":"Un algorithme peut-il être objectif?",
    "longDescription":"La question de l'inclusivité est fondamentale dans la conception d'un algorithme quand celui-ci traite avec des humains. On s'imagine parfois à tort qu'un algorithme est nécessairement plus objectif que la décision d'un opérateur humain (dans la mesure où ses décisions proviendraient d'un ensemble de règles écrites en amont de manière rationnelle et dépassionnée). Or, de nombreuses expériences ont demontré le réel risque de reproduction de travers de société dans certaines prises de décisions algorithmiques (des IA faisant preuve de sexisme, de racisme, d'homophobie...). Plusieurs raisons peuvent expliquer ces comportements. <h2>Données d'entraînements</h2> Comme expliqué dans la partie théorique, les modèles d'aujourd'hui sont le plus souvent concus pour s'adapter au mieux à des données dites \"d'aprentissage\". Cela peut mener deux problèmes: si ces données sont biaisées, l'algorithme le sera tout autant (des données racistes ou sexistes conduiront à un modèle reproduisant celles-ci). Peut-être encore plus problématique, si ces données ne représentent pas la société dans son exhaustivité, il est difficile de prévoir comment l'algorithme se comportera face à des situations qu'il n'aura jamais vu. Plusieurs exemples relatifs aux biais d'apprentissage ont fait la couverture médiatique. <p>En 2016, Microsoft lança un assistant conversationnel sur Twitter, pour qu'il apprenne à converser avec les internautes. En l'espace de 24h, le modèle a progressivement commencé à publier des tweet misanthropes, des appels aux meurtres de militants féministes avant de se réveler ouvertement nazi. Bien entendu, le modèle est lui-même incapable de comprendre ces notions: il ne faisait que répéter ce que les internautes lui ont sciemment appris. </p> <p>La notion de biais n'est pas propre à l'algorithme et celui-ci peut reproduire des discriminations y compris de manière indirecte. En avril 2016, il fut révélé qu'Amazon excluait des quartiers de certains de ses services (livraison en un jour gratuite). Cette décision résultait d'un modèle de bénéfices qui estimait que ces quartiers étaient trop pauvres pour que l'entreprise puisse espérer y faire des profits. Il s'agissait de quartiers essentiellement habités par des populations afro-américaines noires. En ne se basant que sur des données économiques, l'algorithmique avait donc reproduit une discrimination raciale.</p> <p>Bien que renforcé par l'importance que les modèles de machine learning vont prendre, on peut noter que cette notion de biais de données n'est pas spécifique aux applications en IA. Par exemple, en pharmacologie, l'exigence de données représentant la population dans son ensemble (en terme d'ethnicités, de sexe, d'handicap...) est particulièrement importante. </p> <h2></h2>",
    "imgUrl":"assets/ethics/images/gender-bias-person-people-inequality-svgrepo-com.svg",
    "url":"biais"
  },
  {
    "title":"Spécificité de l'éthique",
    "shortDescription":"Un algorithme éthique?",
    "longDescription":"La conception d'algorithmes pouvant prendre des décisions capitales en autonomie interroge sur la notion d'éthique de la machine. Autrement dit, il s'agit de formaliser la prise de décision éthique. Par exemple, quelle prise de décision doit-être celle de la voiture autonome face à un accident inéluctable? Faut-il protéger à tous pris les passages de l'habitacle ou bien le piéton? Derrière ce concept, deux problèmes se posent. Tout d'abord, il n'existe pas de garantie que chaque situation aura été prise en compte dans le <b>formalisme éthique</b> codé dans la machine. De nombreux problèmes éthiques concernent des situations inédites pour lesquelles il n'existe pas forcément de préalable sur lesquels baser un jugement. La prise de décision revient alors aux valeurs propres de l'individu, notion qu'il est difficile de définir pour une machine. La seconde difficulté liée à l'éthique formelle revient aux choix des rédacteurs des règles éthiques.",
    "imgUrl":"assets/ethics/images/computer-surveillance-artificial-intelligence-computer-ai-svgrepo-com.svg",
    "url":"specificite"
  },
  {
    "title":"Fragmentation Algorithmique",
    "shortDescription":"Indivu ou collectif?",
    "longDescription":"La notion de fragmentation du collectif est une conséquence indirecte de la performance des algorithmes, en ",
    "imgUrl":"assets/ethics/images/computer-surveillance-artificial-intelligence-computer-ai-svgrepo-com.svg",
    "url":"specificite"
  },


  {
    "title":"Applications",
    "shortDescription":"Comment concevoir des algorithmes inclusifs?",
    "longDescription":"Pariatur ad consectetur proident officia officia. Nisi ad consequat incididunt fugiat adipisicing velit voluptate fugiat nulla cillum. Minim sit eiusmod consequat quis laboris labore.",
    "imgUrl":"assets/ethics/images/aim-svgrepo-com.svg",
    "url":"applications"
  },
  {
    "title":"Autonomie",
    "shortDescription":"Comment concevoir des algorithmes inclusifs?",
    "longDescription":"Culpa ad anim eu deserunt magna do. Qui qui elit ullamco nisi est proident eu amet reprehenderit quis culpa. Officia dolore quis aute minim.",
    "imgUrl":"assets/ethics/images/artificial-intelligence-svgrepo-com.svg",
    "url":"autonomie"
  }
]
